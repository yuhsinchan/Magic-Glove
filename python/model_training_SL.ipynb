{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "QTensor = namedtuple('QTensor', ['tensor', 'scale', 'zero_point'])\n",
    "def quantize_tensor(x, num_bits=8, min_val=None, max_val=None,imposePow=None,roundType='round'):\n",
    "    if imposePow is not None:\n",
    "        scale=2**(-1*imposePow)\n",
    "    else:\n",
    "        if not min_val and not max_val: \n",
    "            min_val, max_val = x.min(), x.max()\n",
    "        if min_val!=max_val:\n",
    "            scale=2**(torch.ceil(torch.log2((max_val-min_val)/(2.**(num_bits)-1.))))\n",
    "        else:\n",
    "            scale=2**(torch.ceil(torch.log2((torch.tensor([1e-4]))/(2.**(num_bits)-1.))))\n",
    "    zero_point=0\n",
    "    q_x = zero_point + x / scale\n",
    "    qmin = (2.**(num_bits-1))*(-1.)\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "    if roundType=='round':\n",
    "        q_x.clamp_(qmin, qmax).round_()\n",
    "    elif roundType=='floor':\n",
    "        q_x.clamp_(qmin, qmax).floor_()\n",
    "    \n",
    "    return QTensor(tensor=q_x, scale=scale, zero_point=zero_point)\n",
    "\n",
    "def dequantize_tensor(q_x):\n",
    "    return q_x.scale * (q_x.tensor.float() - q_x.zero_point)\n",
    "\n",
    "class FakeQuantOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, numBits=4,imposePow=None,roundType='round'):\n",
    "        x = quantize_tensor(w,num_bits=numBits,imposePow=imposePow,roundType=roundType)\n",
    "        x = dequantize_tensor(x)\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # straight through estimator\n",
    "        return grad_output, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = torch.nn.BatchNorm1d(10)\n",
    "conv = nn.Conv1d(8, 10, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.0182,  0.0218, -0.0378],\n",
       "         [-0.0876, -0.1781, -0.0784],\n",
       "         [ 0.0898, -0.1050, -0.1420],\n",
       "         [-0.0070,  0.0931, -0.1632],\n",
       "         [-0.1119, -0.0430,  0.0136],\n",
       "         [ 0.0467,  0.0417, -0.1641],\n",
       "         [-0.0100,  0.1508,  0.1286],\n",
       "         [ 0.0775,  0.0924,  0.2001]],\n",
       "\n",
       "        [[ 0.0254, -0.1487,  0.1017],\n",
       "         [ 0.1181,  0.1975,  0.1414],\n",
       "         [ 0.1320, -0.0695, -0.0754],\n",
       "         [ 0.1980,  0.1950, -0.1130],\n",
       "         [ 0.0822,  0.1052,  0.0684],\n",
       "         [-0.0262, -0.0292,  0.1041],\n",
       "         [ 0.1981,  0.1516, -0.1985],\n",
       "         [-0.0874,  0.1352, -0.0038]],\n",
       "\n",
       "        [[-0.0664, -0.0327, -0.1545],\n",
       "         [-0.1124,  0.0601,  0.1165],\n",
       "         [ 0.1569,  0.1676, -0.0359],\n",
       "         [-0.1363,  0.1313,  0.0373],\n",
       "         [-0.1252, -0.0822, -0.1826],\n",
       "         [ 0.1955,  0.0219, -0.0948],\n",
       "         [ 0.1164, -0.2021,  0.0046],\n",
       "         [ 0.1856, -0.0805, -0.1149]],\n",
       "\n",
       "        [[ 0.1810,  0.0666,  0.1076],\n",
       "         [-0.1714,  0.1264,  0.0257],\n",
       "         [ 0.1789, -0.0246,  0.1096],\n",
       "         [-0.0448,  0.1401, -0.0283],\n",
       "         [-0.0860,  0.1968, -0.0587],\n",
       "         [-0.0257,  0.1936,  0.1268],\n",
       "         [ 0.1331, -0.0373,  0.0059],\n",
       "         [-0.0687,  0.0406, -0.1353]],\n",
       "\n",
       "        [[ 0.1238,  0.1277,  0.0166],\n",
       "         [ 0.1533,  0.1743, -0.0401],\n",
       "         [-0.0601, -0.1699, -0.0287],\n",
       "         [ 0.1247, -0.2000, -0.1375],\n",
       "         [ 0.0873, -0.1281,  0.0104],\n",
       "         [-0.1496,  0.0517,  0.0884],\n",
       "         [ 0.0592,  0.0431,  0.0644],\n",
       "         [ 0.0827,  0.1675,  0.0822]],\n",
       "\n",
       "        [[-0.0602, -0.1823, -0.1330],\n",
       "         [ 0.1511, -0.1848,  0.0911],\n",
       "         [ 0.0964, -0.1793,  0.0721],\n",
       "         [ 0.0217,  0.0633, -0.0455],\n",
       "         [ 0.0966, -0.1167,  0.2016],\n",
       "         [-0.0687,  0.1417, -0.0794],\n",
       "         [-0.0848,  0.1572,  0.1166],\n",
       "         [ 0.1835,  0.1146,  0.0464]],\n",
       "\n",
       "        [[ 0.0999,  0.0324, -0.0919],\n",
       "         [ 0.0588, -0.1427, -0.0507],\n",
       "         [ 0.1997, -0.1953,  0.0439],\n",
       "         [-0.0284,  0.1349, -0.0539],\n",
       "         [ 0.1429, -0.1126,  0.1605],\n",
       "         [-0.1361,  0.0456,  0.0431],\n",
       "         [-0.1251, -0.0334, -0.1923],\n",
       "         [-0.0255, -0.1844, -0.0620]],\n",
       "\n",
       "        [[-0.0418, -0.0264, -0.1372],\n",
       "         [ 0.0833, -0.0530,  0.0619],\n",
       "         [ 0.0792,  0.1159, -0.0826],\n",
       "         [-0.1686, -0.0432, -0.1767],\n",
       "         [-0.1023, -0.1286, -0.1571],\n",
       "         [ 0.0048,  0.0270,  0.1061],\n",
       "         [ 0.1131, -0.0489, -0.0298],\n",
       "         [ 0.1010,  0.1807,  0.0743]],\n",
       "\n",
       "        [[-0.1092,  0.1107, -0.1713],\n",
       "         [-0.0124, -0.1484, -0.1863],\n",
       "         [ 0.1754, -0.0234, -0.0090],\n",
       "         [ 0.0611,  0.0806, -0.1693],\n",
       "         [ 0.1261,  0.1220, -0.1560],\n",
       "         [-0.1548,  0.0409, -0.1851],\n",
       "         [-0.1466,  0.1091, -0.1080],\n",
       "         [-0.2012, -0.1853,  0.0009]],\n",
       "\n",
       "        [[-0.0130,  0.0247,  0.0756],\n",
       "         [-0.0981,  0.1254,  0.1839],\n",
       "         [ 0.0464, -0.0292, -0.2002],\n",
       "         [ 0.1744, -0.0829, -0.0099],\n",
       "         [ 0.0758,  0.0750, -0.0549],\n",
       "         [-0.1195, -0.1986,  0.1460],\n",
       "         [-0.0503, -0.0471, -0.1271],\n",
       "         [-0.0482,  0.1537, -0.2009]]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBN_Quant(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,nBits=8):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.nBits=nBits\n",
    "        super(ConvBN_Quant, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels,out_channels,kernel_size,stride)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.folded = False\n",
    "        self.quantized = False\n",
    "\n",
    "    def forward(self,x):\n",
    "        r=self.bn.weight.unsqueeze(1).unsqueeze(1)\n",
    "        var=self.bn.running_var.unsqueeze(1).unsqueeze(1)\n",
    "        \n",
    "        if not self.folded:\n",
    "            w=self.conv.weight*r/(torch.sqrt(var+self.bn.eps))\n",
    "            b=(self.conv.bias-self.bn.running_mean)*self.bn.weight/(torch.sqrt(self.bn.running_var+self.bn.eps))+self.bn.bias\n",
    "        else:\n",
    "            w=self.conv.weight\n",
    "            b=self.conv.bias\n",
    "        if not self.quantized:        \n",
    "            w=FakeQuantOp.apply(w,self.nBits)\n",
    "            b=FakeQuantOp.apply(b,self.nBits)\n",
    "\n",
    "        x=F.conv1d(x,w,b,self.stride,self.padding)\n",
    "        return x\n",
    "\n",
    "    def fold(self):\n",
    "        r=self.bn.weight.unsqueeze(1).unsqueeze(1)\n",
    "        var=self.bn.running_var.unsqueeze(1).unsqueeze(1)\n",
    "        self.conv.weight.data = self.conv.weight.data*r/(torch.sqrt(var+self.bn.eps))\n",
    "        self.conv.bias.data = (self.conv.bias.data-self.bn.running_mean)*self.bn.weight.data/(torch.sqrt(self.bn.running_var+self.bn.eps))+self.bn.bias.data\n",
    "        self.folded = True\n",
    "\n",
    "    def foldAndQuantize(self):\n",
    "        self.fold()\n",
    "        self.quantized = True\n",
    "        self.conv.weight.data = FakeQuantOp.apply(self.conv.weight.data,self.nBits)\n",
    "        self.conv.bias.data = FakeQuantOp.apply(self.conv.bias.data,self.nBits)\n",
    "\n",
    "class Linear_Quant(nn.Module):\n",
    "    def __init__(self,in_features,out_features,nBits=8):\n",
    "        super(Linear_Quant,self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.nBits = nBits\n",
    "        self.fc = nn.Linear(in_features,out_features)\n",
    "        self.quantized = False\n",
    "\n",
    "    def forward(self,x):\n",
    "        w = self.fc.weight\n",
    "        b = self.fc.bias\n",
    "        if not self.quantized:\n",
    "            w = FakeQuantOp.apply(w,self.nBits)\n",
    "            b = FakeQuantOp.apply(b,self.nBits)\n",
    "        x=F.linear(x,w,b)\n",
    "        return x\n",
    "\n",
    "    def quantize(self):\n",
    "        self.quantized = True\n",
    "        self.fc.weight.data = FakeQuantOp.apply(self.fc.weight.data,self.nBits)\n",
    "        self.fc.bias.data = FakeQuantOp.apply(self.fc.bias.data,self.nBits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSL_quant(nn.Module):\n",
    "    def __init__(self,numNeurons=32,nBits=4):\n",
    "        super(ModelSL_quant, self).__init__()\n",
    "\n",
    "        self.numNeurons = numNeurons\n",
    "        self.nBits = nBits\n",
    "\n",
    "        self.conv1 = ConvBN_Quant(8,10,3,stride=1,padding=0,nBits=nBits)#output 3*13*13\n",
    "        self.fc1 = Linear_Quant(10 * 3, 80 ,nBits=nBits)\n",
    "        self.fc2 = Linear_Quant(80, 27,nBits=nBits)\n",
    "        self.dropout=nn.Dropout(p=0.2)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x=F.relu6(x)\n",
    "        x=FakeQuantOp.apply(x,self.nBits,None,'floor')\n",
    "\n",
    "        x=x.view(-1, 10*3)\n",
    "        x=self.dropout(x)\n",
    "        x=self.fc1(x)\n",
    "        # x=F.relu6(x)\n",
    "        x=FakeQuantOp.apply(x,self.nBits,None,'floor')\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        # x=F.relu6(x)\n",
    "        x=FakeQuantOp.apply(x,self.nBits,None,'floor')\n",
    "        \n",
    "        # x=self.fc3(x)\n",
    "        # x=FakeQuantOp.apply(x,self.nBits,self.nBits-4,'floor')\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def foldAndQuantize(self):\n",
    "        self.conv1.foldAndQuantize()\n",
    "        self.fc1.quantize()\n",
    "        # self.fc2.quantize()\n",
    "        # self.fc3.quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class NMSLDataset(Dataset):\n",
    "    def __init__(self, data_path: str):\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        speakers = os.listdir(data_path)\n",
    "\n",
    "        for i in speakers:\n",
    "            labeled_data_files = os.listdir(os.path.join(data_path, i))\n",
    "            for file in labeled_data_files:\n",
    "                with open(os.path.join(data_path, i, file), \"r\") as f:\n",
    "                    for line in f.readlines()[:-1]:\n",
    "                        line = line.strip().split(\" \")\n",
    "                        v = [int(i) for i in line[:]]\n",
    "\n",
    "                        data.append(v)\n",
    "                        if file[0] in [chr(ord(\"A\") + i) for i in range(26)]:\n",
    "                            labels.append(torch.tensor(ord(file[0]) - ord(\"A\")))\n",
    "                        elif \"halt\" in file:\n",
    "                            labels.append(torch.tensor(26))\n",
    "                        elif \"random\" in file:\n",
    "                            labels.append(torch.tensor(27))\n",
    "\n",
    "        data = np.array(data)\n",
    "        # normalize data\n",
    "        self.mean = np.int16(np.mean(data, axis=0))\n",
    "        self.std = np.int16(np.std(data, axis=0))\n",
    "        data = (data - np.int16(np.mean(data, axis=0))) / np.int16(np.std(data, axis=0))\n",
    "        # data = (data - np.mean(data, axis=0))\n",
    "\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        this_data = []\n",
    "        this_label = -1\n",
    "        for i, v in enumerate(data):\n",
    "            if len(this_data) < 5:\n",
    "                this_data.append(v)\n",
    "                this_label = labels[i]\n",
    "            else:\n",
    "                self.data.append(torch.tensor(this_data, dtype=torch.float).T)\n",
    "                self.labels.append(torch.tensor(this_label))\n",
    "\n",
    "                if labels[i] != this_label:\n",
    "                    this_data = []\n",
    "                    this_label = -1\n",
    "                    continue\n",
    "                else:\n",
    "                    this_data.pop(0)\n",
    "                    this_data.append(v)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4384/2195322645.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels.append(torch.tensor(this_label))\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"batch_size\": 16,\n",
    "}\n",
    "\n",
    "train_set = NMSLDataset(\"../dataset/train\")\n",
    "train_loader = DataLoader(train_set, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "model = ModelSL_quant(nBits=8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eval_quantize import Classifier\n",
    "\n",
    "# model_fp32 = Classifier()\n",
    "# model_fp32.eval()\n",
    "\n",
    "# model_fp32.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "# model_fp32_fused = torch.quantization.fuse_modules(\n",
    "#     model_fp32, [[\"conv\", \"batchnorm\"]]\n",
    "# )\n",
    "# model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# model_fp32_prepared.load_state_dict(torch.load(\"models/model_quantize.pth\"))\n",
    "\n",
    "# model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# model.conv1.conv.weight = torch.nn.Parameter(torch.dequantize(model_int8.conv.weight()))\n",
    "# torch.dequantize(model_int8.conv.weight()) * model_int8.conv.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fc1.fc.weight = model_continuous.fc[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 847/847 [00:02<00:00, 363.63it/s, loss=781]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.09587423426083105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 847/847 [00:02<00:00, 360.53it/s, loss=534]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.11425197431544763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 847/847 [00:02<00:00, 360.98it/s, loss=1.59e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.11779467119344601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 847/847 [00:02<00:00, 402.67it/s, loss=298]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.11166875784190715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 847/847 [00:02<00:00, 384.12it/s, loss=337]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.1177208650084877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 847/847 [00:02<00:00, 375.16it/s, loss=414]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.08679607351096022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 847/847 [00:02<00:00, 373.75it/s, loss=219]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.09838364454941324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 847/847 [00:02<00:00, 361.66it/s, loss=61.7]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.08834600339508451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 847/847 [00:02<00:00, 374.04it/s, loss=42.2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.09690752085024724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 847/847 [00:02<00:00, 366.97it/s, loss=1.73e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.10089305483799542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 847/847 [00:02<00:00, 379.73it/s, loss=199]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.0895269023544173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 847/847 [00:02<00:00, 379.96it/s, loss=121]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.10140969813270352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 847/847 [00:02<00:00, 365.92it/s, loss=140]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.10318104657170271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14:  31%|███▏      | 265/847 [00:00<00:01, 352.67it/s, loss=50.4]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22646f726d4465736b746f70227d/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22646f726d4465736b746f70227d/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22646f726d4465736b746f70227d/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tepoch:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22646f726d4465736b746f70227d/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         inputs, labels \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22646f726d4465736b746f70227d/home/yuhsinchan/Documents/dclab_2022_fall/final/python/model_training_SL.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/general_slu/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, train_pred = torch.max(outputs, 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc += (train_pred.cpu() == labels.cpu()).sum().item()\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            tepoch.set_postfix(loss=train_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Train Accuracy: {train_acc / len(train_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"quant_unnorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 7.4431,  7.6046,  7.4241],\n",
       "         [ 1.1268,  1.0168,  0.1720],\n",
       "         [ 2.4845,  3.3358,  4.4562],\n",
       "         [-2.5527, -2.6465, -2.7437],\n",
       "         [-7.6747, -7.4491, -7.4075],\n",
       "         [ 8.4138,  8.3789,  8.3266],\n",
       "         [-5.4497, -5.6498, -5.5687],\n",
       "         [ 3.7767,  3.5384,  3.7582]],\n",
       "\n",
       "        [[ 0.7080,  0.5924,  0.9628],\n",
       "         [ 0.4951,  0.3349,  0.4743],\n",
       "         [-0.9440, -0.3481, -0.0550],\n",
       "         [ 2.1597,  2.0484,  1.7948],\n",
       "         [ 1.4867,  1.2428,  1.3726],\n",
       "         [ 2.9807,  2.8316,  2.5310],\n",
       "         [-0.5800, -1.0828, -0.6880],\n",
       "         [-8.4748, -8.2104, -8.5670]],\n",
       "\n",
       "        [[-5.0519, -4.7862, -4.3220],\n",
       "         [-3.0193, -1.8639, -1.4457],\n",
       "         [ 2.3664,  2.8255,  2.3234],\n",
       "         [ 0.7296,  1.1839,  1.4363],\n",
       "         [ 0.6585,  0.9863,  1.4566],\n",
       "         [ 0.1180,  0.0963,  0.2170],\n",
       "         [-4.7629, -4.8583, -4.6756],\n",
       "         [ 1.8902,  1.8657,  1.5236]],\n",
       "\n",
       "        [[ 8.6610,  8.3986,  8.9391],\n",
       "         [-3.2035, -1.4230, -3.0423],\n",
       "         [ 5.4708,  5.0514,  5.1137],\n",
       "         [-3.4865, -3.4358, -3.8167],\n",
       "         [-0.1243, -0.2399, -0.1424],\n",
       "         [-3.8295, -3.7593, -3.9775],\n",
       "         [ 0.9811,  0.4619,  0.4474],\n",
       "         [ 3.0385,  3.1453,  3.0304]],\n",
       "\n",
       "        [[ 0.3656,  0.3643,  0.7343],\n",
       "         [ 2.5951,  2.6112,  1.6970],\n",
       "         [ 1.0906,  1.0140,  1.5425],\n",
       "         [-2.3933, -1.8415, -1.9654],\n",
       "         [-1.7000, -1.8027, -1.4717],\n",
       "         [-1.5728, -1.6654, -1.8333],\n",
       "         [-2.5358, -3.0282, -3.2489],\n",
       "         [ 0.9618,  1.2557,  1.0316]],\n",
       "\n",
       "        [[ 0.4642,  0.8419,  0.5741],\n",
       "         [ 1.1101, -0.3735, -1.0703],\n",
       "         [-0.7630, -0.6599, -0.7921],\n",
       "         [ 2.9686,  3.0491,  3.2965],\n",
       "         [ 1.4655,  1.2338,  1.4705],\n",
       "         [-1.0332, -1.1875, -0.9014],\n",
       "         [-3.9442, -3.9808, -4.1797],\n",
       "         [-6.1562, -5.5389, -5.8913]],\n",
       "\n",
       "        [[-1.9273, -1.5223, -1.0146],\n",
       "         [-1.2324, -0.6642,  0.3436],\n",
       "         [-2.1903, -2.0199, -2.2461],\n",
       "         [ 1.4599,  1.0575,  1.0363],\n",
       "         [ 1.8474,  1.8995,  1.6956],\n",
       "         [-4.6551, -5.0465, -4.7898],\n",
       "         [ 0.9853,  0.9597,  1.1194],\n",
       "         [ 1.2743,  1.5397,  1.3553]],\n",
       "\n",
       "        [[ 1.5748,  2.2627,  2.7063],\n",
       "         [-1.9117, -1.2866, -1.7064],\n",
       "         [-2.1936, -2.6737, -3.0780],\n",
       "         [ 0.7285,  0.3544,  0.4035],\n",
       "         [-4.5822, -4.1051, -4.2011],\n",
       "         [-0.3227, -0.7051, -0.4304],\n",
       "         [ 4.3986,  4.5197,  4.5738],\n",
       "         [-1.6078, -1.1203, -1.3490]],\n",
       "\n",
       "        [[ 2.9559,  2.9356,  3.0708],\n",
       "         [-0.6335,  0.4151,  0.3954],\n",
       "         [ 0.0723, -0.5570, -1.0414],\n",
       "         [-0.5903, -0.1747,  0.1672],\n",
       "         [ 5.5773,  5.7651,  5.5833],\n",
       "         [-3.3756, -2.9769, -3.2815],\n",
       "         [ 1.7416,  1.3366,  1.5097],\n",
       "         [-1.7634, -1.4935, -1.2786]],\n",
       "\n",
       "        [[ 4.1878,  4.3180,  4.7636],\n",
       "         [-2.3439, -1.6861, -2.6234],\n",
       "         [ 1.3004,  1.1733,  1.2949],\n",
       "         [ 1.9562,  2.5861,  2.2860],\n",
       "         [ 0.6889,  0.8055,  0.8454],\n",
       "         [-0.9091, -0.9270, -0.8550],\n",
       "         [-2.8100, -2.7547, -3.0304],\n",
       "         [-2.1487, -1.9492, -1.9197]]], requires_grad=True)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eval import Classifier\n",
    "\n",
    "model_continuous = Classifier()\n",
    "model_continuous.load_state_dict(torch.load(\"./models/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.conv.weight =  model_continuous.cnn[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  9.9009,   6.7030,  12.8561, -17.1859, -15.4959, -14.5686,  10.5488,\n",
       "           9.6615,  11.1175,  13.5918,  12.2274,  12.0412, -13.4970, -10.8626,\n",
       "          -7.1948,   8.3417,   8.2107,  12.0162,  -9.4247,  -9.1299,  -9.3204,\n",
       "          -7.0088,  -6.6695,  -4.7617,  20.4395,  20.8245,  19.5263, -21.5311,\n",
       "         -22.3276, -24.5059],\n",
       "        [ -9.8423,  -8.2556,  -8.6066,  18.7477,  18.0189,  18.0065, -13.7677,\n",
       "         -10.0600, -12.6918, -28.6141, -27.7226, -27.3027,   5.0098,   4.6335,\n",
       "           1.5405,   4.2506,   6.7063,   5.2010,   8.0726,   5.3810,   2.6762,\n",
       "          18.4038,  16.9606,  17.1018,  -2.3210,  -3.7809,  -4.4687,   2.4210,\n",
       "           2.4838,   3.7457],\n",
       "        [-11.5728,  -8.5998, -15.1555, -15.2280, -13.8639, -15.0411, -10.5546,\n",
       "          -9.7625, -12.5296,   7.7000,   7.4855,   9.8387, -10.7691,  -9.3761,\n",
       "         -10.3291, -11.7320,  -6.8195, -12.5887, -11.6014, -10.7547, -13.4409,\n",
       "          -1.7092,   0.1532,  -3.4158,  14.4693,  13.4489,  15.8693,  -5.2792,\n",
       "         -12.6020,  -5.8853],\n",
       "        [ -8.1609,  -6.3920, -10.4525,   1.8390,  -2.0581,  -0.7826,   3.9671,\n",
       "           7.5088,  13.7650,  25.8715,  24.1450,  26.0638,   8.9444,  -2.3887,\n",
       "           7.1581,   9.5681,   9.2427,  20.0857,  21.6184,  21.5231,  22.6825,\n",
       "          -9.8256, -11.8624,  -6.5445,   9.9580,   7.2041,   7.1526,  30.9893,\n",
       "          31.4963,  30.1592],\n",
       "        [-10.2094,  -9.9946, -10.1122, -13.9220, -17.2198, -19.4077,   4.2347,\n",
       "           2.4227,   1.0781,   6.2335,   4.0646,   2.6915,   3.6772,  -2.4256,\n",
       "          -7.3713,  11.9576,   9.1385,   7.4626,  -4.1257, -11.1623, -11.7476,\n",
       "           6.3981,   1.8567,   2.5452,  12.3683,  12.2760,  12.2732, -15.5170,\n",
       "         -13.5139, -12.8361],\n",
       "        [  4.0067,   2.5767,   4.7282,   7.7963,   7.7009,   9.5021,   2.6168,\n",
       "           1.3222,   7.0189, -20.5223, -18.8868, -18.3945,   5.9537,   6.0273,\n",
       "           7.0778, -13.6994, -12.9735, -10.5991, -20.4725, -19.4502, -17.5052,\n",
       "           4.1624,   3.0522,   5.7900, -12.1922, -11.1167, -11.6737, -12.8677,\n",
       "          -8.4350, -11.8030],\n",
       "        [ -6.6461,  -2.6373,  -2.1492,   5.6724,   4.7278,   4.2252, -20.5886,\n",
       "         -16.3538, -15.3307, -18.4936, -17.0821, -16.8850,  -5.6021,  -5.2630,\n",
       "          -5.1422,  -8.6127,  -8.5718,  -9.2422, -21.8948, -22.9520, -24.0098,\n",
       "           5.3086,   5.4552,   6.6337,  -1.5907,  -1.7112,  -0.1969, -17.6350,\n",
       "         -19.6252, -20.9363],\n",
       "        [-14.0259, -11.1161, -14.4395,   4.2480,   1.1073,  -0.2963, -17.2993,\n",
       "         -11.4699, -11.3870,   4.5801,   5.4174,  12.2978,   7.0554,   3.7318,\n",
       "           3.9651,  24.0847,  24.7461,  23.6969,  28.0373,  27.0065,  25.7094,\n",
       "         -11.5093,  -8.7128,  -8.8061,  -2.5482,  -3.1052,  -2.0034,  15.1755,\n",
       "          17.9292,  20.7181],\n",
       "        [-14.1728,  -5.3220, -13.4845, -12.7040, -12.7737, -12.8200, -17.1265,\n",
       "         -13.9139, -16.3968, -11.6139, -15.4113, -12.5943,  -4.8917,  -8.7824,\n",
       "          -6.0203,   0.5046,  11.8386,   5.5409,  10.3261,   2.0521,  -9.0368,\n",
       "           3.1589,   8.8287,   5.9416,   4.2304,   1.5687,   7.3985, -25.5705,\n",
       "         -22.6349, -17.1767],\n",
       "        [ 11.0536,  -8.1447,   4.9303, -18.9431, -16.0094, -15.6879, -19.2181,\n",
       "         -20.2424, -22.1730, -16.8357,  -9.8870, -21.0706, -28.6793,  -5.9409,\n",
       "          -7.4735,   7.6013,  -2.1527,   4.9295,   0.8723,   5.9220,  16.9139,\n",
       "           4.9522,   2.9870,   4.7632,   2.6464,   4.7937,   0.9973, -15.3064,\n",
       "         -24.4006, -32.8000],\n",
       "        [-13.9060, -13.9530, -13.4784,  -1.7102,  -2.5460,  -3.0325, -16.1289,\n",
       "         -14.9587, -15.0999, -18.9987, -15.9461, -18.3914,  -4.3225,   2.3656,\n",
       "           2.6777,   9.8152,   9.9839,  10.7463,  28.7059,  28.5431,  28.6310,\n",
       "          11.2161,  10.6392,   9.8085,  -4.3942,  -4.0640,  -3.0043,  43.2592,\n",
       "          43.2189,  43.3859],\n",
       "        [  5.3976,   4.7062,   5.1506,  10.8794,  10.8929,   7.1416,  -2.6927,\n",
       "          -4.8687,  -8.2382, -16.2053, -17.6069, -19.0066,   2.9828,   2.6510,\n",
       "          -2.3161,  -8.5684, -10.4022, -17.0319,  -4.8175,  -4.9426,  -6.3732,\n",
       "          -7.7809,  -7.2617, -12.1946,  -8.2057,  -8.7202,  -7.9190,  -4.6557,\n",
       "          -3.8134,   0.1895],\n",
       "        [ 22.8157,  21.5647,  20.8730,  23.4508,  22.1481,  22.2795, -36.6734,\n",
       "         -36.8776, -37.2627,  10.9948,  12.6630,  12.7983, -30.7476, -30.3882,\n",
       "         -30.3422,   0.5260,   0.0630,  -0.2685,  17.0122,  15.0472,  16.0151,\n",
       "          26.6804,  26.1733,  25.3511, -15.2164, -14.0207, -13.8212,   3.1286,\n",
       "           3.3346,   3.7198],\n",
       "        [ 23.5678,  23.2842,  23.5920,  21.4784,  22.1560,  23.0236,  -4.9752,\n",
       "          -5.3414,  -6.1292,  20.1758,  18.3481,  18.5398,  -1.8548,  -2.2527,\n",
       "          -2.2538, -21.2495, -20.7382, -20.3970,  13.1448,  15.3129,  15.4579,\n",
       "         -14.1498, -13.0638, -12.2096, -17.3160, -17.7603, -17.9995, -24.0895,\n",
       "         -24.1919, -25.1868],\n",
       "        [ -8.0601,  -6.5077,  -4.6822, -11.1436, -10.3102,  -9.1121,  -9.2351,\n",
       "          -9.0637,  -8.3581,   9.3966,   5.1932,   2.1005,  -9.9149, -10.2066,\n",
       "         -10.2641,  -6.6574,  -3.7586,  -0.7397, -11.1402,  -7.3832,  -7.2797,\n",
       "           1.2359,   3.1575,   4.8186,  19.5494,  18.5820,  17.6561, -18.3563,\n",
       "         -20.5234, -22.3908],\n",
       "        [ -1.5936,  -3.6803,  -4.6750,  22.3536,  20.5118,  17.3056,  21.9931,\n",
       "          20.9511,  19.9787, -19.1567, -18.7208, -19.1809,  -8.2434,  -9.1894,\n",
       "         -12.9711,  -4.5046,  -3.8101,  -4.6141, -16.8597, -15.7942, -16.5880,\n",
       "           7.3742,   7.8304,   7.1096, -15.5385, -14.0835, -12.1607,  21.9594,\n",
       "          21.3232,  22.0105],\n",
       "        [-20.6482, -15.0453, -17.3348,  15.0679,  14.9268,  14.0912,   3.5247,\n",
       "           4.4580,   4.3068, -16.3151, -16.6820, -15.9602,  10.2296,   8.7808,\n",
       "           8.4152,   3.4619,   5.0022,   0.8968,  26.6687,  23.2162,  21.6812,\n",
       "          -9.2393,  -9.5766,  -9.7279, -14.9199, -14.8912, -14.4325, -25.7289,\n",
       "         -24.0220, -22.2874],\n",
       "        [  3.8369,   4.8244,   1.3655,  15.8352,  16.2856,  15.3823,  16.9106,\n",
       "          17.7455,  17.6609,  24.6099,  20.4520,  19.9830,  14.5419,  15.1177,\n",
       "          16.2170,  -9.4110,  -4.6336,  -7.4392,  -6.1406,  -4.8374,  -7.3604,\n",
       "          -9.0475,  -9.2499, -13.3782, -16.1560, -16.9385, -16.0211,  -2.7859,\n",
       "          -6.0554,  -3.4330],\n",
       "        [-10.6587, -10.0148, -12.9646, -20.2725, -18.0528, -17.7576,  -6.7735,\n",
       "          -4.5393,  -4.4392,   6.5019,   8.6223,  14.0021, -11.3281,  -7.3173,\n",
       "          -5.2428,   8.2419,   9.6386,   9.2427, -10.5597,  -9.6854, -11.5297,\n",
       "           3.6906,   4.2028,   3.6793,  17.6819,  18.0884,  18.2471, -16.9903,\n",
       "         -17.2534, -15.1642],\n",
       "        [  6.5918,   3.4693,   2.3348, -12.6766, -14.7745, -15.6797,  -3.9163,\n",
       "          -6.4826,  -4.0778,  29.0852,  28.7531,  29.3008,   6.0092,   5.0300,\n",
       "           5.0859,  14.7627,   9.9395,  10.1736,  -1.1447,  -2.6767,  -1.5720,\n",
       "          -2.3134,  -4.8137,  -4.9630,  12.0582,  12.3242,  12.2933,  -4.1259,\n",
       "          -0.6612,  -0.9825],\n",
       "        [  3.4911,   4.3036,   5.7617,  17.4812,  15.9142,  15.3450,  19.9987,\n",
       "          18.2087,  18.4426,  16.2540,  12.1842,  13.1345,  13.6903,  13.1496,\n",
       "          13.3136,  -8.9451, -11.5349, -10.8814,  -2.0422,  -2.7624,  -2.6637,\n",
       "          -1.3848,  -4.6216,  -5.8266,  -4.2106,  -3.7068,  -3.7001,  -7.7686,\n",
       "          -6.5313,  -7.0979],\n",
       "        [  7.8847,   3.9969,   2.8893,  10.2989,  11.5062,  12.7582,  25.1097,\n",
       "          25.2482,  26.0266,   1.7390,   5.1715,   6.3628,   9.5180,   9.4352,\n",
       "           9.2100,  -9.4890, -10.3215, -10.0167,  -4.7098,  -4.0101,  -3.4455,\n",
       "          -9.7194,  -7.6342,  -4.9767, -10.1197, -11.3807, -12.4435,  -4.5915,\n",
       "          -5.7505,  -7.5893],\n",
       "        [ -2.9109,  -4.3557,  -1.6306,  14.2150,  16.0018,  18.9023,  19.8213,\n",
       "          20.0000,  21.0047, -17.7308, -17.9557, -17.8097,  -9.1177,  -7.1930,\n",
       "          -4.0588,  -7.8920,  -8.5055,  -7.9336, -19.3644, -19.8155, -19.7287,\n",
       "           6.7617,   6.7673,   7.7390,  -2.4843,  -2.5824,  -3.3530,  20.8869,\n",
       "          20.9407,  21.3864],\n",
       "        [ 15.1120,   8.8889,  20.9133, -17.7499, -16.3083, -15.8280,  20.0107,\n",
       "          14.5546,  11.5291,  12.2768,  13.0796,  11.8018,  -1.3572,  -3.0692,\n",
       "          -5.3818,  12.2063,   7.5817,   8.7533,  11.2317,  12.0229,  15.8519,\n",
       "           6.9892,   2.4316,   7.9165,   9.4169,   6.2897,  -4.5135, -18.0101,\n",
       "         -15.9617, -17.8469],\n",
       "        [  9.7798,  -3.7132,   2.3269, -13.1353, -18.6691, -17.4532, -15.1553,\n",
       "         -15.5876, -11.7052, -15.7533, -12.7892, -10.1678,  -2.8339,  -8.5469,\n",
       "          -6.7019,  18.2048,   6.8443,  13.7644,   6.3477,   0.3973,   3.6629,\n",
       "          -3.4910,  -4.7869,  -2.0897,   2.6721,   3.5761,   2.7504, -26.4131,\n",
       "         -23.6464, -26.1033],\n",
       "        [ -5.9454,  -6.5150,   0.4158, -15.1060, -11.1370,  -8.4995,   9.6429,\n",
       "           3.4534,  -1.4846,  22.4623,  23.1204,  21.2634,   1.5179,  12.0642,\n",
       "           7.2882,  -6.7138,  -9.0671,  -9.7400,  19.3653,  24.3900,  25.7929,\n",
       "          -4.8782,  -3.8208,  -5.8952,  -6.4759,  -3.3154,  -4.7182,  24.5940,\n",
       "          23.7187,  24.1156],\n",
       "        [  5.2458,   7.1141,   4.9117,   0.1372,   1.3405,  -2.3631, -24.3691,\n",
       "         -24.2435, -24.5720,  -6.9162,  -6.8857,  -7.8733, -23.8380, -23.6227,\n",
       "         -24.5764, -14.4495, -13.8885, -16.8436,   0.2464,   0.9035,   0.1479,\n",
       "          -8.1626,  -7.9783,  -9.1039, -29.8453, -30.8794, -30.6589,  12.0161,\n",
       "          12.0585,  15.3515]], requires_grad=True)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_continuous.fc[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-1.2500, -1.0000, -0.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-1.7500, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 0.7500,  0.7500,  0.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-1.2500, -1.2500, -0.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -1.7500, -2.0000]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-0.5000, -0.2500, -0.2500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-0.7500, -0.5000, -0.2500],\n",
       "         [ 1.7500,  0.7500, -1.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.2500,  1.0000,  1.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-1.5000, -1.0000, -0.5000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.0000,  1.0000,  1.2500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 0.0000,  0.0000,  0.2500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-1.0000, -1.5000, -1.5000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-1.2500, -1.0000, -0.7500],\n",
       "         [-2.0000, -2.0000, -2.0000]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.5000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-1.0000, -1.5000, -1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-1.5000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 0.7500,  0.5000,  0.2500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-0.7500, -1.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000]],\n",
       "\n",
       "        [[-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.0000, -0.2500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.2500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-1.7500, -0.5000,  0.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[ 0.5000,  0.2500,  0.5000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500]],\n",
       "\n",
       "        [[-1.5000, -1.5000, -1.5000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-1.5000, -1.7500, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [ 1.7500,  1.7500,  1.7500],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000],\n",
       "         [-2.0000, -2.0000, -2.0000]]], grad_fn=<FakeQuantOpBackward>)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1(batch[0])\n",
    "FakeQuantOp.apply(x,4,2,'floor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  1.8261,   2.2174,   2.5727],\n",
       "         [  6.2744,   6.3523,   6.4094],\n",
       "         [ -1.1992,  -0.8839,  -0.5984],\n",
       "         [  6.9401,   7.0403,   7.0215],\n",
       "         [ -1.6119,  -2.8514,  -3.6193],\n",
       "         [-24.9754, -23.9446, -23.6868],\n",
       "         [  2.2261,   2.3035,   1.9894],\n",
       "         [  3.1311,   1.9863,   2.0373],\n",
       "         [ 22.8820,  21.4625,  20.4743],\n",
       "         [ 13.8359,  13.4738,  13.7487]],\n",
       "\n",
       "        [[ -6.7216,  -6.6231,  -6.4130],\n",
       "         [  7.2416,   7.0602,   6.9100],\n",
       "         [  0.8252,   0.9631,   0.9869],\n",
       "         [ -8.3983,  -8.9000,  -9.0575],\n",
       "         [-19.6867, -20.1361, -20.1205],\n",
       "         [ -2.4598,  -2.5005,  -2.3612],\n",
       "         [ -1.0218,  -1.1869,  -0.6536],\n",
       "         [  7.3385,   9.7405,   9.8739],\n",
       "         [ 36.9013,  36.9882,  37.3386],\n",
       "         [ -1.8702,  -1.5086,  -2.3235]],\n",
       "\n",
       "        [[  2.2631,   2.3896,   2.3993],\n",
       "         [  6.6419,   6.6683,   6.5779],\n",
       "         [ -0.3333,  -0.1865,  -0.2282],\n",
       "         [  5.8046,   5.8779,   6.2832],\n",
       "         [ -9.3882, -10.1488,  -9.5151],\n",
       "         [-18.2789, -17.4125, -17.0168],\n",
       "         [ -0.6095,  -0.2665,  -0.2069],\n",
       "         [  1.9624,   0.9198,  -0.9565],\n",
       "         [ 16.9599,  16.2890,  15.2978],\n",
       "         [  6.8854,   6.7756,   6.3515]],\n",
       "\n",
       "        [[  4.4228,   4.4704,   4.4177],\n",
       "         [  1.3553,   1.2334,   1.1947],\n",
       "         [ 11.9840,  12.0191,  11.8569],\n",
       "         [-15.8161, -15.7610, -15.7346],\n",
       "         [ -1.2636,  -0.8365,  -0.3707],\n",
       "         [-11.6157, -11.3512, -11.9805],\n",
       "         [ -3.3750,  -3.1887,  -3.5635],\n",
       "         [  9.0562,   8.4153,   9.2246],\n",
       "         [ 33.2633,  32.7582,  33.1742],\n",
       "         [  9.5741,   9.4462,   9.5766]],\n",
       "\n",
       "        [[  2.6607,   2.6613,   2.4782],\n",
       "         [  4.8776,   4.9243,   5.0064],\n",
       "         [  3.9050,   3.9525,   3.8288],\n",
       "         [  6.9203,   6.8807,   6.8333],\n",
       "         [-32.1969, -32.5593, -33.1985],\n",
       "         [ 17.2977,  17.7252,  18.2678],\n",
       "         [  1.0615,   1.0845,   1.3363],\n",
       "         [-17.2584, -17.5518, -17.6956],\n",
       "         [-18.1082, -18.3276, -18.1571],\n",
       "         [  2.2846,   2.1643,   1.9591]],\n",
       "\n",
       "        [[  4.3924,   4.5326,   4.4983],\n",
       "         [ -5.3059,  -5.2259,  -5.4029],\n",
       "         [  0.0734,   0.1928,   0.4565],\n",
       "         [ -8.3626,  -8.4728,  -9.2079],\n",
       "         [ 11.6822,  11.4395,  11.5150],\n",
       "         [ -5.4232,  -5.9811,  -5.6003],\n",
       "         [ -0.9544,  -1.3153,  -1.3106],\n",
       "         [ 28.8795,  29.9478,  30.6273],\n",
       "         [ -1.1262,  -0.9748,  -0.5514],\n",
       "         [ -2.6658,  -2.1877,  -2.3499]],\n",
       "\n",
       "        [[  2.7484,   2.9557,   3.0395],\n",
       "         [  3.9144,   4.0521,   4.1672],\n",
       "         [  1.9757,   2.0438,   2.0808],\n",
       "         [ -3.8111,  -3.7188,  -3.8211],\n",
       "         [  1.8749,   1.9162,   1.5234],\n",
       "         [-25.0278, -26.1229, -26.4823],\n",
       "         [ -0.8574,  -1.3275,  -1.5566],\n",
       "         [ 19.1587,  19.9251,  20.5917],\n",
       "         [ 25.2274,  25.9454,  25.9973],\n",
       "         [ 17.5887,  17.7796,  18.1859]],\n",
       "\n",
       "        [[  2.2337,   2.2127,   2.2685],\n",
       "         [ -3.1740,  -3.2431,  -3.3267],\n",
       "         [ -3.2816,  -2.8847,  -2.8927],\n",
       "         [ -1.3395,  -2.2252,  -2.1858],\n",
       "         [ 35.1339,  34.6603,  34.5627],\n",
       "         [-10.8329,  -9.9114,  -9.5055],\n",
       "         [ -6.8267,  -6.7158,  -6.6189],\n",
       "         [-22.4413, -22.5410, -22.8665],\n",
       "         [-29.6035, -29.4384, -30.0030],\n",
       "         [ 12.9781,  12.7782,  12.6412]],\n",
       "\n",
       "        [[  0.7687,   0.5715,   0.4892],\n",
       "         [ -6.2288,  -6.0100,  -5.7139],\n",
       "         [ -3.8840,  -3.9333,  -3.9417],\n",
       "         [ -2.9757,  -3.1509,  -3.1252],\n",
       "         [ -0.5096,  -0.9998,  -1.9971],\n",
       "         [ 16.6613,  17.2756,  17.9251],\n",
       "         [ 12.2506,  12.7738,  13.1202],\n",
       "         [ 17.2905,  15.6508,  14.0248],\n",
       "         [ -7.2082,  -6.7816,  -6.8513],\n",
       "         [-14.2878, -14.2542, -14.4260]],\n",
       "\n",
       "        [[ -6.6647,  -6.8489,  -6.9472],\n",
       "         [  6.0027,   6.1047,   6.2640],\n",
       "         [ -7.6324,  -7.6629,  -7.4962],\n",
       "         [  8.8819,   9.2267,   9.4348],\n",
       "         [  2.3632,   1.2116,  -0.0876],\n",
       "         [-24.2584, -24.1022, -23.9326],\n",
       "         [-10.8295, -11.1631, -11.0612],\n",
       "         [ 16.0159,  17.5231,  19.0968],\n",
       "         [ 21.2784,  21.4471,  21.4405],\n",
       "         [  1.2767,   2.0206,   2.6905]],\n",
       "\n",
       "        [[ -3.2496,  -3.0711,  -2.9264],\n",
       "         [  3.8052,   3.8894,   3.9996],\n",
       "         [ -3.7168,  -3.5146,  -3.6870],\n",
       "         [  4.3055,   3.9073,   4.3138],\n",
       "         [ -5.2193,  -5.7331,  -5.7009],\n",
       "         [ -6.0849,  -6.4971,  -7.2505],\n",
       "         [ 14.5009,  14.1796,  14.2390],\n",
       "         [ -1.5333,  -0.3760,   0.0710],\n",
       "         [ 29.0808,  29.1380,  29.3058],\n",
       "         [  3.3906,   3.9970,   4.2155]],\n",
       "\n",
       "        [[  0.5557,   0.4470,   0.5244],\n",
       "         [ -4.7005,  -4.6810,  -4.6230],\n",
       "         [ -5.5324,  -5.6657,  -5.7735],\n",
       "         [  3.4993,   3.7993,   4.1178],\n",
       "         [ 36.8316,  36.6431,  37.3446],\n",
       "         [ -2.1478,  -1.8496,  -2.6258],\n",
       "         [  2.8808,   3.3036,   2.9433],\n",
       "         [-33.1428, -33.4797, -33.9367],\n",
       "         [-33.8218, -33.5848, -34.1109],\n",
       "         [ 13.8131,  13.5202,  14.1680]],\n",
       "\n",
       "        [[ -1.4459,  -1.3494,  -1.2966],\n",
       "         [ -2.0553,  -1.9933,  -1.9677],\n",
       "         [  5.3449,   5.4751,   5.4823],\n",
       "         [ -1.4804,  -1.7029,  -1.7676],\n",
       "         [-43.1252, -43.4683, -43.3008],\n",
       "         [ 52.8674,  52.4625,  52.1355],\n",
       "         [ -2.8803,  -3.2192,  -3.4777],\n",
       "         [ -3.8820,  -2.6102,  -2.4831],\n",
       "         [-45.0394, -44.6951, -44.7927],\n",
       "         [-11.7070, -11.4781, -11.3651]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([773, -88, 257, 318, 324, 334, 340, 307], dtype=int16)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([497, 253, 272,  19,  45,  43,  36,  41], dtype=int16)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_data = max([torch.max(i) for i in train_set.data])\n",
    "min_data = min([torch.min(i) for i in train_set.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.0435), tensor(-3.8972))"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_data, min_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.7772, grad_fn=<MaxBackward1>),\n",
       " tensor(1.9489, grad_fn=<StdBackward0>),\n",
       " tensor(0.3777, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model.conv1(batch[0])), torch.std(model.conv1(batch[0])), torch.mean(model.conv1(batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0318, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(model.conv1.conv.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 3])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-30.3771, -28.3105, -30.9589, -30.7700, -19.0844, -27.0349, -27.8618,\n",
       "        -22.6335, -29.6849, -28.1582], requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('general_slu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5661c73bf5f4ce6179e3148e3c825b6832fd4cb82045a10b114db5b9548c77c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
